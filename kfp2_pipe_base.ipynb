{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:automl"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to use the Vertex AI Pipelines with KFP 2.x.\n",
    "\n",
    "Learn more about Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:pipelines,automl"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn to use `Vertex AI Pipelines` and KFP 2.x version of `Google Cloud Pipeline Components` to train and deploy an XGBoost model.\n",
    "\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Pipelines`\n",
    "- `Google Cloud Pipeline Components`\n",
    "- `BigQuery`\n",
    "\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a KFP pipeline:\n",
    "    - Create a `BigQuery Dataset` resource.\n",
    "    - Export the dataset.\n",
    "    - Train an XGBoost `Model` resource.\n",
    "    - Create an `Endpoint` resource.\n",
    "    - Deploys the `Model` resource to the `Endpoint` resource.\n",
    "- Compile the KFP pipeline.\n",
    "- Execute the KFP pipeline using `Vertex AI Pipelines`\n",
    "\n",
    "The components are [documented here](https://google-cloud-pipeline-components.readthedocs.io/en/latest/google_cloud_pipeline_components.aiplatform.html#module-google_cloud_pipeline_components.aiplatform)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aef4f59195ad"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The [Census Income Data Set](https://archive.ics.uci.edu/ml/datasets/Census+Income) that this notebook uses for training is available publicly at the BigQuery location `bigquery-public-data.ml_datasets.census_adult_income`. It consists of the following fields:\n",
    "\n",
    "- `age`: Age.\n",
    "- `workclass`: Nature of employment.\n",
    "- `functional_weight`: Sample weight of the individual from the original Census data. How likely they were to be included in this dataset, based on their demographic characteristics vs. whole-population estimates.\n",
    "- `education`: Level of education completed.\n",
    "- `education_num`: Estimated years of education completed based on the value of the education field.\n",
    "- `marital_status`: Marital status.\n",
    "- `occupation`: Occupation category.\n",
    "- `relationship`: Relationship to the household.\n",
    "- `race`: Race.\n",
    "- `sex`: Gender.\n",
    "- `capital_gain`: Amount of capital gains.\n",
    "- `capital_loss`: Amount of capital loss.\n",
    "- `hours_per_week`: Hours worked per week.\n",
    "- `native_country`: Country of birth.\n",
    "- `income_bracket`: Either \" >50K\" or \" <=50K\" based on income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0316df526f8"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F9dJ5Of-dORl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ubernetes (/opt/conda/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: kfp>2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (1.81.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (8.1.8)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (0.16)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (2.21.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (2.38.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (2.19.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.6.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (0.6.0)\n",
      "Requirement already satisfied: kfp-server-api<2.4.0,>=2.1.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (2.3.0)\n",
      "Requirement already satisfied: kubernetes<31,>=8.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (30.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (4.25.6)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp>2) (1.26.20)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.26.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.2)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.26.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.5)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.7)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.10.6)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (1.67.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.70.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp>2) (1.6.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp>2) (1.17.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp>2) (2025.1.31)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (3.2.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.27.2)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (2.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp>2) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.10)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ubernetes (/opt/conda/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ubernetes (/opt/conda/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip3 install --no-cache-dir --upgrade \"kfp>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd35dabc167e"
   },
   "source": [
    "Check the KFP SDK version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NN0mULkEeb84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.11.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ubernetes (/opt/conda/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mgoogle-cloud-aiplatform==1.81.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea812d229d23"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a61e2fd1dc68"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# if \"google.colab\" in sys.modules:\n",
    "\n",
    "#     import IPython\n",
    "\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb6faf50d3cd"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b055958409c"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0c74b83ece6c"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# if \"google.colab\" in sys.modules:\n",
    "\n",
    "#     from google.colab import auth\n",
    "\n",
    "#     auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbc07d616504"
   },
   "source": [
    "### Set Google Cloud project information \n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "LOCATION = \"us-central1\"\n",
    "LOCATION = \"us-central1\"\n",
    "BQ_LOCATION = LOCATION.split(\"-\")[0].upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "autoset_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "91c46850b49b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://my-project-0004-346516-unique/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'my-project-0004-346516-unique' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $LOCATION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85c4ecfd133a"
   },
   "source": [
    "#### Service Account \n",
    "\n",
    "You use a service account to create Vertex AI Pipeline jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "77b01a1fdbb4"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "f936bebda2d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 255766800726-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40ef6967cad3"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step. You only need to run this step once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "f88cb0488c08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://my-project-0004-346516-unique/\n",
      "No changes made to gs://my-project-0004-346516-unique/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "V0aexAES_cnZ"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "## Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VB_XJHA3iccD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/envs/tensorflow/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n",
      "PIPELINE_ROOT: gs://my-project-0004-346516-unique/census_pipeline\n"
     ]
    }
   ],
   "source": [
    "PATH = %env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "\n",
    "DATASET_ID = \"census\"  # The Data Set ID where the view sits\n",
    "VIEW_NAME = \"census_data\"  # BigQuery view you create for input data\n",
    "\n",
    "# KFP_ENDPOINT = (\n",
    "#     \"https://720c5bc00c3d6089-dot-us-central1.pipelines.googleusercontent.com/\"\n",
    "# )\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/census_pipeline\"  # This is where all pipeline artifacts are sent. You'll need to ensure the bucket is created ahead of time\n",
    "PIPELINE_ROOT\n",
    "print(f\"PIPELINE_ROOT: {PIPELINE_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "052ca6eeecaf"
   },
   "source": [
    "### Create a BigQuery dataset\n",
    "\n",
    "Next, you create a BQ dataset for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wzehWtGpBIes"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Dataset 'my-project-0004-346516:census' already\n",
      "exists.\n"
     ]
    }
   ],
   "source": [
    "# Create a BQ Dataset in the project.\n",
    "!bq mk --location=$BQ_LOCATION --dataset $PROJECT_ID:$DATASET_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ce90503cfe4"
   },
   "source": [
    "## Define components for the pipeline\n",
    "\n",
    "Next, you define several components that you use in your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65183ad63691"
   },
   "source": [
    "### Define create BigQuery dataset component\n",
    "\n",
    "First, you define a component to create BigQuery dataset view from the public dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ac414f17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/kfp/dsl/component_decorator.py:126: FutureWarning: The default base_image used by the @dsl.component decorator will switch from 'python:3.9' to 'python:3.10' on Oct 1, 2025. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.10.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery==3.10.0\"],\n",
    ")\n",
    "def create_census_view(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    view_name: str,\n",
    "):\n",
    "    \"\"\"Creates a BigQuery view on `bigquery-public-data.ml_datasets.census_adult_income`.\n",
    "\n",
    "    Args:\n",
    "        project_id: The Project ID.\n",
    "        dataset_id: The BigQuery Dataset ID. Must be pre-created in the project.\n",
    "        view_name: The BigQuery view name.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    create_or_replace_view = \"\"\"\n",
    "        CREATE OR REPLACE VIEW\n",
    "        `{dataset_id}`.`{view_name}` AS\n",
    "        SELECT\n",
    "          age,\n",
    "          workclass,\n",
    "          education,\n",
    "          education_num,\n",
    "          marital_status,\n",
    "          occupation,\n",
    "          relationship,\n",
    "          race,\n",
    "          sex,\n",
    "          capital_gain,\n",
    "          capital_loss,\n",
    "          hours_per_week,\n",
    "          native_country,\n",
    "          income_bracket,\n",
    "        FROM\n",
    "          `bigquery-public-data.ml_datasets.census_adult_income`\n",
    "    \"\"\".format(\n",
    "        dataset_id=dataset_id, view_name=view_name\n",
    "    )\n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query=create_or_replace_view, job_config=job_config)\n",
    "    query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94fb3fd6e079"
   },
   "source": [
    "### Define export dataset component\n",
    "\n",
    "Next, you define a component to export the data from your BigQuery dataset to use for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "v9NXQwPlFMgW"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery[pandas]==3.10.0\"],\n",
    ")\n",
    "def export_dataset(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    view_name: str,\n",
    "    dataset: Output[Dataset],\n",
    "):\n",
    "    \"\"\"Exports from BigQuery to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        project_id: The Project ID.\n",
    "        dataset_id: The BigQuery Dataset ID. Must be pre-created in the project.\n",
    "        view_name: The BigQuery view name.\n",
    "\n",
    "    Returns:\n",
    "        dataset: The Dataset artifact with exported CSV file.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    table_name = f\"{project_id}.{dataset_id}.{view_name}\"\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      `{table_name}`\n",
    "    \"\"\".format(\n",
    "        table_name=table_name\n",
    "    )\n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query=query, job_config=job_config)\n",
    "    df = query_job.result().to_dataframe()\n",
    "    df.to_csv(dataset.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8078c3ccf485"
   },
   "source": [
    "### Define XGBoost training component\n",
    "\n",
    "Next, you define a component to train an XGBoost model with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "211c652f"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pip==23.0\",\n",
    "        \"xgboost==1.6.2\",\n",
    "        \"pandas==1.3.5\",\n",
    "        \"numpy==1.23.0\",\n",
    "        \"joblib==1.1.0\",\n",
    "        \"scikit-learn==1.0.2\",     \n",
    "    ],\n",
    ")\n",
    "def xgboost_training(\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "):\n",
    "    \"\"\"Trains an XGBoost classifier.\n",
    "\n",
    "    Args:\n",
    "        dataset: The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        model: The model artifact stores the model.joblib file.\n",
    "        metrics: The metrics of the trained model.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import (accuracy_score, precision_recall_curve,\n",
    "                                 roc_auc_score)\n",
    "    from sklearn.model_selection import (RandomizedSearchCV, StratifiedKFold,\n",
    "                                         train_test_split)\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # Load the training census dataset\n",
    "    with open(dataset.path, \"r\") as train_data:\n",
    "        raw_data = pd.read_csv(train_data)\n",
    "\n",
    "    CATEGORICAL_COLUMNS = (\n",
    "        \"workclass\",\n",
    "        \"education\",\n",
    "        \"marital_status\",\n",
    "        \"occupation\",\n",
    "        \"relationship\",\n",
    "        \"race\",\n",
    "        \"sex\",\n",
    "        \"native_country\",\n",
    "    )\n",
    "    LABEL_COLUMN = \"income_bracket\"\n",
    "    POSITIVE_VALUE = \" >50K\"\n",
    "\n",
    "    # Convert data in categorical columns to numerical values\n",
    "    encoders = {col: LabelEncoder() for col in CATEGORICAL_COLUMNS}\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        raw_data[col] = encoders[col].fit_transform(raw_data[col])\n",
    "\n",
    "    X = raw_data.drop([LABEL_COLUMN], axis=1).values\n",
    "    y = raw_data[LABEL_COLUMN] == POSITIVE_VALUE\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    _ = xgb.DMatrix(X_train, label=y_train)\n",
    "    _ = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    params = {\n",
    "        \"reg_lambda\": [0, 1],\n",
    "        \"gamma\": [1, 1.5, 2, 2.5, 3],\n",
    "        \"max_depth\": [2, 3, 4, 5, 10, 20],\n",
    "        \"learning_rate\": [0.1, 0.01],\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=50,\n",
    "        objective=\"binary:hinge\",\n",
    "        silent=True,\n",
    "        nthread=1,\n",
    "        eval_metric=\"auc\",\n",
    "    )\n",
    "\n",
    "    folds = 5\n",
    "    param_comb = 20\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=params,\n",
    "        n_iter=param_comb,\n",
    "        scoring=\"precision\",\n",
    "        n_jobs=4,\n",
    "        cv=skf.split(X_train, y_train),\n",
    "        verbose=4,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    xgb_model_best = random_search.best_estimator_\n",
    "    predictions = xgb_model_best.predict(X_test)\n",
    "    score = accuracy_score(y_test, predictions)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    _ = precision_recall_curve(y_test, predictions)\n",
    "\n",
    "    metrics.log_metric(\"accuracy\", (score * 100.0))\n",
    "    metrics.log_metric(\"framework\", \"xgboost\")\n",
    "    metrics.log_metric(\"dataset_size\", len(raw_data))\n",
    "    metrics.log_metric(\"AUC\", auc)\n",
    "\n",
    "    # Export the model to a file\n",
    "    os.makedirs(model.path, exist_ok=True)\n",
    "    joblib.dump(xgb_model_best, os.path.join(model.path, \"model.joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "242c3348d54d"
   },
   "source": [
    "### Define deploying the model component\n",
    "\n",
    "Finally, you define a component to deploy the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wMSORMSQvlwO"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform==1.25.0\"],\n",
    ")\n",
    "def deploy_xgboost_model(\n",
    "    model: Input[Model],\n",
    "    project_id: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model],\n",
    "):\n",
    "    \"\"\"Deploys an XGBoost model to Vertex AI Endpoint.\n",
    "\n",
    "    Args:\n",
    "        model: The model to deploy.\n",
    "        project_id: The project ID of the Vertex AI Endpoint.\n",
    "\n",
    "    Returns:\n",
    "        vertex_endpoint: The deployed Vertex AI Endpoint.\n",
    "        vertex_model: The deployed Vertex AI Model.\n",
    "    \"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project_id)\n",
    "\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"census-demo-model\",\n",
    "        artifact_uri=model.uri,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-6:latest\",\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb7d14f7d3db"
   },
   "source": [
    "## Construct the XGBoost training pipeline\n",
    "\n",
    "Now you define the pipeline, with the following steps:\n",
    "\n",
    "- Create a BigQuery view of the dataset.\n",
    "- Export the dataset.\n",
    "- Train the model.\n",
    "- Deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "df55e79c"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"census-demo-pipeline\",\n",
    ")\n",
    "def pipeline():\n",
    "    \"\"\"A demo pipeline.\"\"\"\n",
    "\n",
    "    create_input_view_task = create_census_view(\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        view_name=VIEW_NAME,\n",
    "    )\n",
    "\n",
    "    export_dataset_task = (\n",
    "        export_dataset(\n",
    "            project_id=PROJECT_ID,\n",
    "            dataset_id=DATASET_ID,\n",
    "            view_name=VIEW_NAME,\n",
    "        )\n",
    "        .after(create_input_view_task)\n",
    "        .set_caching_options(False)\n",
    "    )\n",
    "\n",
    "    training_task = xgboost_training(\n",
    "        dataset=export_dataset_task.outputs[\"dataset\"],\n",
    "    )\n",
    "\n",
    "    _ = deploy_xgboost_model(\n",
    "        project_id=PROJECT_ID,\n",
    "        model=training_task.outputs[\"model\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_train_pipeline:icn"
   },
   "source": [
    "### Compile the pipeline\n",
    "\n",
    "Next, you compile the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4abfd490"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lz99NXISmL0I"
   },
   "source": [
    "### Run the pipeline using Vertex AI Pipelines\n",
    "\n",
    "Now, run the compiled pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "c2cd77f23d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/255766800726/locations/us-central1/pipelineJobs/census-demo-pipeline-20250225154726\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/255766800726/locations/us-central1/pipelineJobs/census-demo-pipeline-20250225154726')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/census-demo-pipeline-20250225154726?project=255766800726\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/census-demo-pipeline-20250225154726 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/census-demo-pipeline-20250225154726 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/census-demo-pipeline-20250225154726 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/census-demo-pipeline-20250225154726 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/census-demo-pipeline-20250225154726 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/census-demo-pipeline-20250225154726 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/census-demo-pipeline-20250225154726 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/255766800726/locations/us-central1/pipelineJobs/census-demo-pipeline-20250225154726\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"census-demo-pipeline\",\n",
    "    template_path=\"pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38c373adbe83"
   },
   "source": [
    "### Run the pipeline using KFP\n",
    "\n",
    "Alternatively, you can run the pipeline using KFP directly.\n",
    "\n",
    "*Note:* You need to provide your own value for `KFP_ENDPOINT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24c8f4c2aec4"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "kfp2_pipeline.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-env-tensorflow-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
